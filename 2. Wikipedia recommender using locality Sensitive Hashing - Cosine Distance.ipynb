{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf012445",
   "metadata": {},
   "source": [
    "# Locality Sensitive Hashing (LSH) - Cosine Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258c7b0",
   "metadata": {},
   "source": [
    "Locality Sensitive Hashing (LSH) provides for a fast, efficient approximate nearest neighbor search. The algorithm scales well with respect to the number of data points as well as dimensions.\n",
    "\n",
    "In this notebook, we will\n",
    "* Implement the LSH algorithm for approximate nearest neighbor search\n",
    "* Examine the accuracy for different documents by comparing against brute force search, and also contrast runtimes\n",
    "* Explore the role of the algorithmâ€™s tuning parameters in the accuracy of the method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553f726",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a7e6953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # to conform python 2.x print to python 3.x\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import time\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a53b61",
   "metadata": {},
   "source": [
    "## Load in the Wikipedia dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "25d8ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../people_wiki_csv/people_wiki.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db13e4",
   "metadata": {},
   "source": [
    "Taking subset of given dataset as corpus creation of TF-IDF is time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ceb732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=20000)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba47f03",
   "metadata": {},
   "source": [
    "For this assignment, let us assign a unique ID to each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7c4aabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c9dbb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   URI     20000 non-null  object\n",
      " 1   name    20000 non-null  object\n",
      " 2   text    20000 non-null  object\n",
      " 3   id      20000 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 625.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Susan_King_Borcha...</td>\n",
       "      <td>Susan King Borchardt</td>\n",
       "      <td>susan king borchardt born susan king on july 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Justin_Lowe_Quack...</td>\n",
       "      <td>Justin Lowe Quackenbush</td>\n",
       "      <td>justin lowe quackenbush born 1929 is a united ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Arthur_Anslyn&gt;</td>\n",
       "      <td>Arthur Anslyn</td>\n",
       "      <td>captain roy arthur anslyn mbe formally known a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Roderick_Hietbrink&gt;</td>\n",
       "      <td>Roderick Hietbrink</td>\n",
       "      <td>roderick hietbrink gorssel 1975 is a contempor...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Kazuhiro_Wada&gt;</td>\n",
       "      <td>Kazuhiro Wada</td>\n",
       "      <td>kazuhiro wada wada kazuhiro born june 19 1972 ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URI                     name  \\\n",
       "0  <http://dbpedia.org/resource/Susan_King_Borcha...     Susan King Borchardt   \n",
       "1  <http://dbpedia.org/resource/Justin_Lowe_Quack...  Justin Lowe Quackenbush   \n",
       "2        <http://dbpedia.org/resource/Arthur_Anslyn>            Arthur Anslyn   \n",
       "3   <http://dbpedia.org/resource/Roderick_Hietbrink>       Roderick Hietbrink   \n",
       "4        <http://dbpedia.org/resource/Kazuhiro_Wada>            Kazuhiro Wada   \n",
       "\n",
       "                                                text  id  \n",
       "0  susan king borchardt born susan king on july 2...   0  \n",
       "1  justin lowe quackenbush born 1929 is a united ...   1  \n",
       "2  captain roy arthur anslyn mbe formally known a...   2  \n",
       "3  roderick hietbrink gorssel 1975 is a contempor...   3  \n",
       "4  kazuhiro wada wada kazuhiro born june 19 1972 ...   4  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5684fc",
   "metadata": {},
   "source": [
    "Creating word cospus of dataset using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "391ed767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x257709 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2739119 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    min_df=0,\n",
    "    stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(df['text'])\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "361201c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar item id:  1\n",
      "cosine similarity:  1.0\n",
      "item description:  justin lowe quackenbush born 1929 is a united states federal judge on the united states district court for the eastern district of washingtonquackenbush was born in spokane washington his father carl quackenbush was a law student who eventually became a superior court judge in spokanequackenbush received a ba from the university of idaho in 1951 he received an llb from gonzaga university school of law his fathers alma mater in 1957 he was in the united states navy from 1951 to 1954 he was a deputy prosecuting attorney in spokane county washington from 1957 to 1959 he was in private practice in spokane from 1959 until his judicial nomination he was active in democratic party politics regularly serving as the campaign manager for tom foleys successful congressional election campaigns starting in 1964 for over a decadequackenbush also taught at gonzaga university school of law from 1961 to 1967 and was an active masonon may 9 1980 president jimmy carter nominated quackenbush to the seat vacated by marshall a neill he was confirmed by the united states senate on june 18 1980 and received his commission the same day because neill was the only judge in the district and had died in october 1979 quackenbush and fellow appointee robert j mcnichols immediately faced a tremendous backlog of caseshe served as chief judge from 1989 to june 27 1995 when he assumed senior status the annual quackenbush lecture series at gonzaga university school of law is named in his honor\n",
      "\n",
      "similar item id:  16591\n",
      "cosine similarity:  0.2679799063701842\n",
      "item description:  jean constance hamilton born 1945 is a senior united states district judge of the united states district court for the eastern district of missouriborn in st louis missouri hamilton received a ba from wellesley college in 1968 then a jd from washington university school of law in 1971 and an llm from yale law school in 1982 she was an attorney with the civil rights division of the united states department of justice in washington dc from 1971 to 73 she was then an assistant united states attorney for the eastern district of missouri from 1973 to 1978 she was corporate counsel to southwestern bell telephone company in st louis from 1978 to 1981in 1982 hamilton became a circuit judge for missouris twentysecond judicial circuit and in 1988 she was elevated to the missouri court of appeals eastern district during her service as a state court judge hamilton was an adjunct professor teaching at st louis university law school from 1986 to 1987 and in 1989 and at washington university school of law from 1987 to 1992on august 3 1990 president george h w bush nominated hamilton to a seat on the united states district court for the eastern district of missouri vacated by john f nangle she was confirmed by the united states senate on september 28 1990 and received her commission on october 1 1990 she served as chief judge from 1995 to 2002 she took senior status on july 1 2013\n",
      "\n",
      "similar item id:  3213\n",
      "cosine similarity:  0.25137530135644304\n",
      "item description:  carol e jackson born 1952 is a united states federal judgeborn in st louis missouri jackson received a ba from wellesley college in 1973 followed by a jd from the university of michigan law school in 1976 she was in private practice in st louis from 1976 to 1983 and was then a senior attorney of mallinckrodt inc in st louis from 1983 to 1985 in 1986 jackson was hired by the united states district court for the eastern district of missouri to be a united states magistrate judge a position which allowed her to carry out many functions of a federal judge but not requiring an appointment by the president of the united states during this time she was also an adjunct professor at the washington university school of law from 1989 to 1992on april 1 1992 jackson was nominated by president george h w bush to a seat on the eastern district of missouri vacated by william l hungate she was confirmed by the united states senate on august 12 1992 and received her commission on august 17 1992 she became the first woman to serve as a district court judge in the eastern district of missouri she served as chief judge from 2002 through 2009\n",
      "\n",
      "similar item id:  5845\n",
      "cosine similarity:  0.23828615138816178\n",
      "item description:  claudia ann wilken born 1949 is a senior united states federal judge of the united states district court for the northern district of californiaborn in minneapolis minnesota wilken received a ba from stanford university in 1971 and a jd from the university of california berkeley school of law boalt hall in 1975 she was a staff attorney of federal public defenders office northern district of california from 1975 to 1978 she was in private practice in berkeley california from 1978 to 1984 she was an adjunct professor university of california boalt hall school of law from 1978 to 1984 she was a professor new college school of law from 1980 to 1985wilken is the chief federal judge on the united states district court for the northern district of california wilken was nominated by president bill clinton on october 7 1993 to a new seat created by 104 stat 5089 she was confirmed by the united states senate on november 20 1993 and received her commission on november 22 1993 she served as chief judge from august 23 2012 until december 16 2014 at which time she assumed senior statuswilken was formerly a united states magistrate judge for the us district court for the northern district of california 19831993\n",
      "\n",
      "similar item id:  13659\n",
      "cosine similarity:  0.23748738858897522\n",
      "item description:  wiley young daniel born 1946 is a senior united states district judge for the united states district court for the district of coloradoborn in louisville kentucky daniel received a ba from howard university in 1968 and a jd from howard university school of law in 1971 he was in private practice in detroit michigan from 1971 to 1977 and was also a director of wayne county neighborhood legal services from 1974 to 1976 and an adjunct professor at the detroit college of law from 1974 to 1977 in 1977 he moved his private practice to denver colorado and became an adjunct professor at the university of colorado school of law where he continued teaching until 1980 he was also a director of colorados personnel services board from 1979 to 1983 and was a director and vicechair of the iliff school of theology in 1983on march 31 1995 daniel was nominated by president bill clinton to a seat on the united states district court for the district of colorado vacated by sherman g finesilver daniel was confirmed by the united states senate on june 30 1995 and received his commission the same day he served as chief judge from 2008 to 2012 he took senior status on january 1 2013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_similarity_items(X_tfidf, item_id, topn=5):\n",
    "    \"\"\"\n",
    "    Get the top similar items for a given item id.\n",
    "    The similarity measure here is based on cosine distance.\n",
    "    \"\"\"\n",
    "    query = X_tfidf[item_id]\n",
    "    scores = X_tfidf.dot(query.T).toarray().ravel()\n",
    "    best_docs_idx = np.argpartition(scores, -topn)[-topn:]\n",
    "    return sorted(zip(best_docs_idx, scores[best_docs_idx]), key=lambda x: -x[1])\n",
    "\n",
    "similar_items = get_similarity_items(X_tfidf, item_id=1)\n",
    "\n",
    "# an item is always most similar to itself, in real-world\n",
    "# scenario we might want to filter itself out from the output\n",
    "for similar_item, similarity in similar_items:\n",
    "    item_description = df.loc[similar_item, 'text']\n",
    "    print('similar item id: ', similar_item)\n",
    "    print('cosine similarity: ', similarity)\n",
    "    print('item description: ', item_description)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983ebf7",
   "metadata": {},
   "source": [
    "## Getting Started with LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "20033603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_vectors(dim, n_vectors):\n",
    "    \"\"\"\n",
    "    generate random projection vectors\n",
    "    the dims comes first in the matrix's shape,\n",
    "    so we can use it for matrix multiplication.\n",
    "    \"\"\"\n",
    "    return np.random.randn(dim, n_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "26a1084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  257709\n",
      "dimension:  (257709, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.12167502,\n",
       "         0.44386323,  0.33367433],\n",
       "       [ 1.49407907, -0.20515826,  0.3130677 , ...,  1.46935877,\n",
       "         0.15494743,  0.37816252],\n",
       "       [-0.88778575, -1.98079647, -0.34791215, ..., -0.4380743 ,\n",
       "        -1.25279536,  0.77749036],\n",
       "       ...,\n",
       "       [-1.27563281, -0.02830168, -0.33329509, ...,  0.05493989,\n",
       "         0.03536699,  0.27375141],\n",
       "       [ 2.30330495, -2.07363189,  2.6134636 , ..., -0.93946163,\n",
       "        -0.65873327,  1.64827779],\n",
       "       [-0.71556917, -0.4479988 ,  0.01104397, ..., -0.53365767,\n",
       "        -0.43076083, -1.70195993]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tfidf.get_feature_names_out())\n",
    "print('vocabulary size: ', vocab_size)\n",
    "\n",
    "np.random.seed(0)\n",
    "n_vectors = 16\n",
    "random_vectors = generate_random_vectors(vocab_size, n_vectors)\n",
    "print('dimension: ', random_vectors.shape)\n",
    "random_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cd698",
   "metadata": {},
   "source": [
    "Next, we'd like to decide which bin each documents should go. Since 16 random vectors were generated in the previous cell, we have 16 bits to represent the bin index. The first bit is given by the sign of the dot product between the first random vector and the document's TF-IDF vector, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c0d1767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension:  (1, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False,  True, False, False, False,\n",
       "        False,  True,  True, False,  True,  True,  True]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use one data point's tfidf representation as an example\n",
    "data_point = X_tfidf[0]\n",
    "\n",
    "# True if positive sign; False if negative sign\n",
    "bin_indices_bits = data_point.dot(random_vectors) >= 0\n",
    "print('dimension: ', bin_indices_bits.shape)\n",
    "bin_indices_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b64b88e",
   "metadata": {},
   "source": [
    "All documents that obtain exactly this vector will be assigned to the same bin. One further preprocessing step we'll perform is to convert this resulting bin into integer representation. This makes it convenient for us to refer to individual bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "06ab84f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32768 16384  8192  4096  2048  1024   512   256   128    64    32    16\n",
      "     8     4     2     1]\n",
      "[33847]\n"
     ]
    }
   ],
   "source": [
    "bin_indices_bits = data_point.dot(random_vectors) >= 0\n",
    "\n",
    "# https://wiki.python.org/moin/BitwiseOperators\n",
    "# x << y is the same as multiplying x by 2 ** y\n",
    "powers_of_two = 1 << np.arange(n_vectors - 1, -1, step=-1)\n",
    "print(powers_of_two)\n",
    "\n",
    "# final integer representation of individual bins\n",
    "bin_indices = bin_indices_bits.dot(powers_of_two)\n",
    "print(bin_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be34a93",
   "metadata": {},
   "source": [
    "We can repeat the identical operation on all documents in our dataset and compute the corresponding bin. We'll again leverage matrix operations so that no explicit loop is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "76f80406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can do it for the entire corpus\n",
    "bin_indices_bits = X_tfidf.dot(random_vectors) >= 0\n",
    "print(bin_indices_bits.shape)\n",
    "bin_indices = bin_indices_bits.dot(powers_of_two)\n",
    "bin_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96037807",
   "metadata": {},
   "source": [
    "bin_indices represent the bin index number for all documents in corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de9830",
   "metadata": {},
   "source": [
    "Now we are ready to complete the following function. Given the integer bin indices for the documents, we would curate the list of document IDs that belong to each bin. Since a list is to be maintained for each unique bin index, a dictionary of lists is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c450742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_lsh(X_tfidf, n_vectors, seed=None):    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    dim = X_tfidf.shape[1]\n",
    "    random_vectors = generate_random_vectors(dim, n_vectors)\n",
    "    \n",
    "    # partition data points into bins,\n",
    "    # and encode bin index bits into integers\n",
    "    bin_indices_bits = X_tfidf.dot(random_vectors) >= 0\n",
    "    powers_of_two = 1 << np.arange(n_vectors - 1, -1, step=-1)\n",
    "    bin_indices = bin_indices_bits.dot(powers_of_two)\n",
    "    \n",
    "    # update `table` so that `table[i]` is the list of document ids with bin index equal to i\n",
    "    table = defaultdict(list)\n",
    "    \n",
    "    for idx, bin_index in enumerate(bin_indices):\n",
    "        table[bin_index].append(idx)\n",
    "        \n",
    "    # note that we're storing the bin_indices here\n",
    "    # so we can do some ad-hoc checking with it,\n",
    "    # this isn't actually required\n",
    "    model = {'table': table,\n",
    "             'random_vectors': random_vectors,\n",
    "             'bin_indices': bin_indices,\n",
    "             'bin_indices_bits': bin_indices_bits}\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "n_vectors = 16\n",
    "model = train_lsh(X_tfidf, n_vectors, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1a300",
   "metadata": {},
   "source": [
    "After generating our LSH model, let's examine the generated bins to get a deeper understanding of them. Recall that during the background section, given a product's tfidf vector representation, we were able to find its similar product using standard cosine similarity. Here, we will look at these similar products' bins to see if the result matches intuition. Remember the idea behind LSH is that similar data points will tend to fall into nearby bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "324e3b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits 1:  [ True False  True  True False  True False  True  True False  True  True\n",
      " False False  True  True]\n",
      "bits 2:  [ True False  True  True  True False False False False  True False  True\n",
      " False  True  True  True]\n",
      "Number of agreed bins:  9\n"
     ]
    }
   ],
   "source": [
    "# comparison\n",
    "similar_item_ids = [similar_item for similar_item, _ in similar_items]\n",
    "bits1 = model['bin_indices_bits'][similar_item_ids[0]]\n",
    "bits2 = model['bin_indices_bits'][similar_item_ids[1]]\n",
    "\n",
    "print('bits 1: ', bits1)\n",
    "print('bits 2: ', bits2)\n",
    "print('Number of agreed bins: ', np.sum(bits1 == bits2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbce478",
   "metadata": {},
   "source": [
    "Looking at the result above, it does seem like LSH is doing what its intended to do: i.e., similar data points will agree upon more bin indices than dissimilar data points, however, in a high-dimensional space such as text features, we can get unlucky with our selection of only a few random vectors such that dissimilar data points go into the same bin while similar data points fall into different bins. Hence, given a query document, we should consider all documents in the nearby bins and sort them according to their actual distances from the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354f068",
   "metadata": {},
   "source": [
    "## Querying the LSH model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f6107",
   "metadata": {},
   "source": [
    "Let us first implement the logic for searching nearby neighbors, which goes like this:\n",
    "```\n",
    "1. Let L be the bit representation of the bin that contains the query documents.\n",
    "2. Consider all documents in bin L.\n",
    "3. Consider documents in the bins whose bit representation differs from L by 1 bit.\n",
    "4. Consider documents in the bins whose bit representation differs from L by 2 bits.\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5dbdf1",
   "metadata": {},
   "source": [
    "To obtain candidate bins that differ from the query bin by some number of bits, we use `itertools.combinations`, which produces all possible subsets of a given list. See [this documentation](https://docs.python.org/3/library/itertools.html#itertools.combinations) for details.\n",
    "```\n",
    "1. Decide on the search radius r. This will determine the number of different bits between the two vectors.\n",
    "2. For each subset (n_1, n_2, ..., n_r) of the list [0, 1, 2, ..., num_vector-1], do the following:\n",
    "   * Flip the bits (n_1, n_2, ..., n_r) of the query bin to produce a new bit vector.\n",
    "   * Fetch the list of documents belonging to the bin indexed by the new bit vector.\n",
    "   * Add those documents to the candidate set.\n",
    "```\n",
    "\n",
    "Each line of output from the following cell is a 3-tuple indicating where the candidate bin would differ from the query bin. For instance,\n",
    "```\n",
    "(0, 1, 3)\n",
    "```\n",
    "indicates that the candiate bin differs from the query bin in first, second, and fourth bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "916d5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def search_nearby_bins(query_bin_bits, table, search_radius=3, candidate_set=None):\n",
    "    \"\"\"\n",
    "    For a given query vector and trained LSH model's table\n",
    "    return all candidate neighbors with the specified search radius.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    model = train_lsh(X_tfidf, n_vectors=16, seed=0)\n",
    "    query = model['bin_index_bits'][0]  # vector for the first document\n",
    "    candidates = search_nearby_bins(query, model['table'])\n",
    "    \"\"\"\n",
    "    if candidate_set is None:\n",
    "        candidate_set = set()\n",
    "\n",
    "    n_vectors = query_bin_bits.shape[0]\n",
    "    powers_of_two = 1 << np.arange(n_vectors - 1, -1, step=-1)\n",
    "    \n",
    "    for different_bits in combinations(range(n_vectors), search_radius):\n",
    "        bits_index = list(different_bits)\n",
    "        alternate_bits = query_bin_bits.copy()\n",
    "        alternate_bits[bits_index] = np.logical_not(alternate_bits[bits_index])\n",
    "        \n",
    "        # convert the new bit vector to an integer index\n",
    "        nearby_bin = alternate_bits.dot(powers_of_two)\n",
    "        \n",
    "        # fetch the list of documents belonging to\n",
    "        # the bin indexed by the new bit vector,\n",
    "        # then add those documents to candidate_set;\n",
    "        # make sure that the bin exists in the table\n",
    "        if nearby_bin in table:\n",
    "            candidate_set.update(table[nearby_bin])\n",
    "            \n",
    "    return candidate_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c2f31b",
   "metadata": {},
   "source": [
    "The next code chunk uses this searching for nearby bins logic to search for similar documents and return a dataframe that contains the most similar data points according to LSH and their corresponding distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1af1e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "def get_nearest_neighbors(X_tfidf, query_vector, model, max_search_radius=3):\n",
    "    table = model['table']\n",
    "    random_vectors = model['random_vectors']\n",
    "    \n",
    "    # compute bin index for the query vector, in bit representation.\n",
    "    bin_index_bits = np.ravel(query_vector.dot(random_vectors) >= 0)\n",
    "    \n",
    "    # search nearby bins and collect candidates\n",
    "    candidate_set = set()\n",
    "    for search_radius in range(max_search_radius + 1):\n",
    "        candidate_set = search_nearby_bins(bin_index_bits, table, search_radius, candidate_set)\n",
    "        \n",
    "    # sort candidates by their true distances from the query\n",
    "    candidate_list = list(candidate_set)\n",
    "    candidates = X_tfidf[candidate_list]\n",
    "    distance = pairwise_distances(candidates, query_vector, metric='cosine').flatten()\n",
    "    \n",
    "    distance_col = 'distance'\n",
    "    nearest_neighbors = pd.DataFrame({\n",
    "        'id': candidate_list, distance_col: distance\n",
    "    }).sort_values(distance_col).reset_index(drop=True)\n",
    "    \n",
    "    return nearest_neighbors\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2d3330db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original similar items:\n",
      "[(1, 1.0), (16591, 0.2679799063701842), (3213, 0.25137530135644304), (5845, 0.23828615138816178), (13659, 0.23748738858897522)]\n",
      "dimension:  (2337, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16494</td>\n",
       "      <td>0.785009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12859</td>\n",
       "      <td>0.794292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17588</td>\n",
       "      <td>0.795483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12652</td>\n",
       "      <td>0.795935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  distance\n",
       "0      1  0.000000\n",
       "1  16494  0.785009\n",
       "2  12859  0.794292\n",
       "3  17588  0.795483\n",
       "4  12652  0.795935"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('original similar items:\\n' + str(similar_items))\n",
    "\n",
    "item_id = 1\n",
    "query_vector = X_tfidf[item_id]\n",
    "nearest_neighbors = get_nearest_neighbors(X_tfidf, query_vector, model, max_search_radius=5)\n",
    "print('dimension: ', nearest_neighbors.shape)\n",
    "nearest_neighbors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f76a94",
   "metadata": {},
   "source": [
    "We can observe from the result above that when we use a max_search_radius of 5, our LSH-based similarity search wasn't capable of retrieving the actual most similar items to our target data point, this is expected as we have mentioned LSH is an approximate nearest neighborhood search method. However, if we were to increase the radius parameter to 12, we can in fact retrieve all the actual most similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f1393955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension:  (19837, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16591</td>\n",
       "      <td>0.732020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3213</td>\n",
       "      <td>0.748625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5845</td>\n",
       "      <td>0.761714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13659</td>\n",
       "      <td>0.762513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  distance\n",
       "0      1  0.000000\n",
       "1  16591  0.732020\n",
       "2   3213  0.748625\n",
       "3   5845  0.761714\n",
       "4  13659  0.762513"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors = get_nearest_neighbors(X_tfidf, query_vector, model, max_search_radius=12)\n",
    "print('dimension: ', nearest_neighbors.shape)\n",
    "nearest_neighbors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217c7a2",
   "metadata": {},
   "source": [
    "In this article, we saw that LSH performs an efficient neighbor search by randomly partitioning all reference data points into different bins, when it comes to the similarity search stage, it will only consider searching within data points that fall within the same bin. Then a radius parameter gives the end-user full control over the trade-off between the speed of the search versus the quality of the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb26222",
   "metadata": {},
   "source": [
    "## REFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d5a435",
   "metadata": {},
   "source": [
    "[Jupyter Notebook: Locality Sensitive Hashing](http://ethen8181.github.io/machine-learning/recsys/content_based/lsh_text.html#Getting-Started-with-LSH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
